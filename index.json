[{"authors":null,"categories":null,"content":" \u0026nbsp;\nWelcome to the StarAi Deep Reinforcement Learning course\n\u0026nbsp;\nThe goal of this course is two fold:\n Most RL courses come at the material from a highly mathematical approach. We aim to explain essential Reinforcement Learning concepts such as value based methods using a fundamentally human tool - stories. We believe what you cannot create, you do not understand. We have provided easy to use exercises, with answers, to reinforce your learning.  \u0026nbsp;\n\u0026nbsp;\nMaximise Your Learning. To maximise your learning, we recommend you form a study group(s) in your local area and cover the course contents over a period of 6 weeks.\nFor inspiration, check out what we did last year here.\n\u0026nbsp;\nExercise Technology. All exercises are executed in Google Colaboratory and run \u0026ldquo;in browser\u0026rdquo; making them hardware independent. All you need is a Google account and either Firefox or Google Chrome Browsers.\n\u0026nbsp;\nHow to do the exercises. Watch the lecture material, then have an attempt at the exercises.\nWe highly recommend you only look at the answers after spending significant time trying to solve the exercise material.\n\u0026nbsp;\nEn Taro Adun! Way back in April 2018! -an actual epoch in the machine learning world!- we set out to create this course to help speed up the development of advanced machine learning techniques through a fun platform - mainly - Starcraft 2, hence the name \u0026ldquo;StarAi\u0026rdquo;.\nFor a long time Starcraft has been considered within the machine learning community to be the next \u0026ldquo;Grand Challenge\u0026rdquo; for Artificial Intelligence due to several properties of the game including very high state \u0026amp; action spaces, partial observability \u0026amp; multi agent gameplay.\nThis year Deepmind made significant progress with regards to solving the game. For all intents and purposes, removing the \u0026ldquo;human limitations\u0026rdquo; restrictions currently emposed on Deepmind\u0026rsquo;s system, the game can be said to be solved and am sure we will see even further progress towards the end of the year from Deepmind\u0026rsquo;s Starcraft team akin to AlphaGo Zero.\nWhy release the course then? We believe that the way we taught the content - through stories - is unique, and a fun way to learn advanced machine learning \u0026amp; reinforcement learning techniques in a \u0026ldquo;human\u0026rdquo; way. We would also be honoured if someone out there takes inspiration from this course that eventually leads to new breakthroughs!\nIn addition, we believe that RL will have massive impact in the real world. That is the subject of the next section!\nFor those of you only interested in the Starcraft element of StarAi please refer to the content of week 6 by clicking this link.\n\u0026nbsp;\nReinforcement Learning in the Wild. Outside of games however, we are beginning to see more use cases of Reinforcement Learning in the wild.\nIncluding web store product recommendations, news recommendation, in the finance industry , Automated Architecture search for Machine Learning using RL ,End to End Machine Learning for Self Driving Cars, and last and most excitingly the field of Robotics where Deep Reinforcement Learning is exploding!\nWe believe that RL will start to play an ever increasing role where automated decision making is required in an end to end manner.\n\u0026nbsp;\nAdditional Learning Material For those technically inclined, or if you would like to dive deeper we have provided links to relevant chapters in Sutton \u0026amp; Bartos\u0026rsquo;s excellent text book, \u0026ldquo;Reinforcement Learning: An Introduction\u0026rdquo; for each week\u0026rsquo;s material.\n\u0026nbsp;\nStarAi is Beta StarAi was entirely created by volunteer experts of their respective fields here in Sydney, Australia.\nWhilst we did our best to ensure the content is as polished as it could be, we admit bugs exist both logical and otherwise.\nIt has also been some time since StarAi was run and some of the exercise content might be out of date (tensorflow 2.0 etc). We would welcome and feedback good or bad. Potential updates to our exercises \u0026amp; codebase would also be much appreciated.\nIf you would like to provide feedback or help in any way please reach out at contact@starai.io\n\u0026nbsp;\nDisclaimer Your use of the StarAi Reinforcement Learning course acknowledges that you have read this Disclaimer.\nThe course is provided \u0026ldquo;as is\u0026rdquo; without warranty of any kind. We assume no responsibility for the use of this courseÂ material and we therefore exclude all liability.\nStarAi is a non-commericial entity. StarAi is not associated associated with Blizzard Entertainment, Google Inc., Microsoft Pty Ltd, Deepmind Technologies Limited, OpenAI LP or any other legal entity mentioned though out the duration of the StarAi course. Any associated trademarks are the property of their respective owners.\nAll rights in and to intellectual property rights, including without limitation, copyright or other proprietary rights, belong to their respective owners. Except where expressly stated otherwise or if such use is expressly licensed, these Terms of Use are governed by Australian law and through international treaties, applicable law in other countries. You hereby acknowledge that you are solely responsible for compliance with those terms, rules, regulations, and notices pertaining to the use of StarAi and the operation of the services provided thereon.\nThis Disclaimer was written, in part, by GPT2 :)\n\u0026nbsp;\nCredits Opening Cinematic in part created by Upheaval ArtsGaming\n","date":1560002400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1560002400,"objectID":"97ede4bb026d52eb5ee887238b8270ea","permalink":"https://starai-course.github.io/course/","publishdate":"2019-06-09T00:00:00+10:00","relpermalink":"/course/","section":"course","summary":"\u0026nbsp;\nWelcome to the StarAi Deep Reinforcement Learning course\n\u0026nbsp;\nThe goal of this course is two fold:\n Most RL courses come at the material from a highly mathematical approach. We aim to explain essential Reinforcement Learning concepts such as value based methods using a fundamentally human tool - stories. We believe what you cannot create, you do not understand. We have provided easy to use exercises, with answers, to reinforce your learning.","tags":null,"title":"StarAi: Deep Reinforcement Learning Course","type":"docs"},{"authors":null,"categories":null,"content":" \u0026nbsp; \u0026nbsp;\nVideo  \u0026nbsp; \u0026nbsp;\nDescription In this lecture, we will take you on a journey into the near future by discussing the recent developments in the field of Reinforcement Learning - by introducing you to what Reinforcement Learning is, how it differs from Deep Learning and the future impact of RL technology.\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  Andrej Karpathy\u0026rsquo;s ConvNetJS Deep Q Learning Demo  \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"fe48f6107cb916060b166080585f82b0","permalink":"https://starai-course.github.io/course/lecture0/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture0/","section":"course","summary":"\u0026nbsp; \u0026nbsp;\nVideo  \u0026nbsp; \u0026nbsp;\nDescription In this lecture, we will take you on a journey into the near future by discussing the recent developments in the field of Reinforcement Learning - by introducing you to what Reinforcement Learning is, how it differs from Deep Learning and the future impact of RL technology.\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  Andrej Karpathy\u0026rsquo;s ConvNetJS Deep Q Learning Demo  \u0026nbsp; \u0026nbsp;","tags":null,"title":"Lecture 0 : Introduction to Reinforcement Learning","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription In this lecture, we introduce you to your very first RL algorithm, Epsilon Greedy. We start off by exploring a toy problem known as the \u0026ldquo;multi armed bandit problem\u0026rdquo; or in english- how to win at slot machines! We then dive down into how Epsilon-Greedy solves the bandit problem, go on a detour introducing OpenAi Gym (and why it is important!) and finally hand you over to your first exercise, solving the bandit problem in OpenAi Gym.\nA written version of this lecture is also available on the StarAi blog.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 1 Epsilon Greedy Lecture slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 1:\nLecture 1: Epsilon-Greedy \u0026amp; the Multi- Armed Bandit with OpenAi Gym\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for lecture 1:\nExercise Solutions: Epsilon-Greedy \u0026amp; the Multi- Armed Bandit with OpenAi Gym\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  Sutton \u0026amp; Barto\u0026rsquo;s Reinforcement Learning: An Introduction - Chapter 2 intro, section 2.1 up to 2.5 Tom Roth\u0026rsquo;s Multiarmed Bandit Simulator - Get a feel for how the Multiarmed Bandit works live in your browser! Edx\u0026rsquo;s Brilliant Python Course - Note that the majority of StarAi\u0026rsquo;s exercises are in the Python programming language. If you would like to further you knowledge in this field we strongly suggest you learn Python.  \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"89634ec17b1cdd242ef9687309c47566","permalink":"https://starai-course.github.io/course/lecture1/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture1/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription In this lecture, we introduce you to your very first RL algorithm, Epsilon Greedy. We start off by exploring a toy problem known as the \u0026ldquo;multi armed bandit problem\u0026rdquo; or in english- how to win at slot machines! We then dive down into how Epsilon-Greedy solves the bandit problem, go on a detour introducing OpenAi Gym (and why it is important!) and finally hand you over to your first exercise, solving the bandit problem in OpenAi Gym.","tags":null,"title":"Lecture 1: Epsilon-Greedy \u0026 the multiarmed bandit problem","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription Disclaimer: This is the most mathematical lecture out of the StarAi series. Whilst we endeavoured to make the StarAi content as accessible as possible, this particular lecture covers the base fundamentals \u0026amp; therefore contains the most formulas. If formulas is not for you please proceed to week 3. If however you would like to dive deeper down the mathematical formulation of the RL framework we highly recommend David Silver\u0026rsquo;s excellent course on Youtube.\nIn this lecture you will learn the fundamentals of Reinforcement Learning. We start off by discussing the Markov environment and its properties, gradually building our understanding of the intuition behind the Markov Decision Process and its elements, like state-value function, action-value function and policies. We then move on to discussing Bellman equations and the intuition behind them. At the end we will explore one of the Bellman equation implementations, using the Dynamic Programming approach and finish with an exercise, where you will implement state-value and action-value functions algorithms and find an optimal policy to solve the Gridworld problem.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 2 Markov Decision Processes slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 2:\nLecture 2 Exercise 1: Policy Evaluation Exercise\nLecture 2 Exercise 2: Policy Iteration Exercise\nLecture 2 Exercise 3: Value Iteration Exercise\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for lecture 2:\nExercise Solutions 1: Policy Evaluation\nExercise Solutions: Value Iteration\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  Sutton \u0026amp; Barto\u0026rsquo;s Reinforcement Learning: An Introduction - All of Chapter 3 \u0026amp; 4.  \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"2b2a3ae8e7b1eff9bd8a6396a33c5bc6","permalink":"https://starai-course.github.io/course/lecture2/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture2/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription Disclaimer: This is the most mathematical lecture out of the StarAi series. Whilst we endeavoured to make the StarAi content as accessible as possible, this particular lecture covers the base fundamentals \u0026amp; therefore contains the most formulas. If formulas is not for you please proceed to week 3. If however you would like to dive deeper down the mathematical formulation of the RL framework we highly recommend David Silver\u0026rsquo;s excellent course on Youtube.","tags":null,"title":"Lecture 2: Markov Decision Processes, Dynamic Programming","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription In this session, participants will focus on a specific method of Temporal Difference Learning called Tabular Q Learning. Participants will learn the theory behind Q Learning, implement the different components bit by bit and combine these components to solve the robot in a maze scenario.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 3 \u0026amp; 4 TabularQ slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 3:\nLecture 3 Exercise: Tabular Q FrozenLake\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for lecture 3:\nExercise Solutions: Tabular Q FrozenLake\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  Sutton \u0026amp; Barto\u0026rsquo;s Reinforcement Learning: An Introduction - Chapter 5 read intro and summary, Chapter 6 intro, section 6.1 to 6.3 + 6.5 (other sections optional but good to read), Chapter 7 read intro and summary  \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"8983df5d8021b56c2fe7296c8ed422bb","permalink":"https://starai-course.github.io/course/lecture3/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture3/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription In this session, participants will focus on a specific method of Temporal Difference Learning called Tabular Q Learning. Participants will learn the theory behind Q Learning, implement the different components bit by bit and combine these components to solve the robot in a maze scenario.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 3 \u0026amp; 4 TabularQ slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 3:","tags":null,"title":"Lecture 3: Tabular Q Gridworld","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription In this session, participants will explore the problem of an environment where observations are continuous variables. Participants will learn the discretisation technique and implement this with the previous components to solve the problem of keeping a cart pole upright without having any understanding of the observations.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 3 \u0026amp; 4 TabularQ slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 4:\nLecture 4 Exercise: Tabular Q Cartpole!\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for lecture 4:\nExercise Solutions: Tabular Q Cartpole\n\u0026nbsp; \u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material \u0026hellip;\n\u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"8844807c1c49d06d779fcdb0551067f1","permalink":"https://starai-course.github.io/course/lecture4/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture4/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription In this session, participants will explore the problem of an environment where observations are continuous variables. Participants will learn the discretisation technique and implement this with the previous components to solve the problem of keeping a cart pole upright without having any understanding of the observations.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 3 \u0026amp; 4 TabularQ slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 4:","tags":null,"title":"Lecture 4:  Tabular Q Cartpole","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription Neural Q-Learning builds on the theory developed in previous sessions, augmenting the tabular Q-Learning algorithm with the powerful function approximation capabilities of Neural Networks. NQL is the \u0026ldquo;base\u0026rdquo; algorithm unifying Neural Networks and Reinforcement Learning, and participants will be exposed to both the impressive generalization properties of this algorithm, as well as some of it\u0026rsquo;s potential drawbacks and limitations.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 5 part 1 Neural Q Theory slides\nStarAi Lecture 5 part 2 Neural Q Implementation slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 5:\nlecture 5 Exercise: Neural Q Learning Exercise\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for lecture 5:\nlecture 5 Exercise: Neural Q Learning Exercise Solutions\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  Sutton \u0026amp; Barto\u0026rsquo;s Reinforcement Learning: An Introduction - Chapter 16 section 16.5  \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"19f48238cee25fd7dd98595c7d6c4fab","permalink":"https://starai-course.github.io/course/lecture5/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture5/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription Neural Q-Learning builds on the theory developed in previous sessions, augmenting the tabular Q-Learning algorithm with the powerful function approximation capabilities of Neural Networks. NQL is the \u0026ldquo;base\u0026rdquo; algorithm unifying Neural Networks and Reinforcement Learning, and participants will be exposed to both the impressive generalization properties of this algorithm, as well as some of it\u0026rsquo;s potential drawbacks and limitations.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 5 part 1 Neural Q Theory slides","tags":null,"title":"Lecture 5:  NQL Theory","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription Deep Q-Networks refer to the method proposed by Deepmind in 2014 to learn to play ATARI2600 games from the raw pixel observations. This hugely influential method kick-started the resurgence in interest in Deep Reinforcement Learning, however it\u0026rsquo;s core contributions deal simply with the stabilization of the NQL algorithm. In these session these key innovations (Experience Replay, Target Networks, and Huber Loss) are stepped though, taking the participants from the relatively unstable NQL algorithm to a fully-implemented DQN.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 6-DQN slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 6:\nlecture 6 Exercise: DQN Homework Exercise\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for lecture 6:\nlecture 6 Exercise: DQN Homework Exercise Solutions\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  DeepRL Bootcamp lecture from Vlad Mnih, one of the original authors of DQN  \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"070ccb71968f6b4c161f114739c995cc","permalink":"https://starai-course.github.io/course/lecture6/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture6/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription Deep Q-Networks refer to the method proposed by Deepmind in 2014 to learn to play ATARI2600 games from the raw pixel observations. This hugely influential method kick-started the resurgence in interest in Deep Reinforcement Learning, however it\u0026rsquo;s core contributions deal simply with the stabilization of the NQL algorithm. In these session these key innovations (Experience Replay, Target Networks, and Huber Loss) are stepped though, taking the participants from the relatively unstable NQL algorithm to a fully-implemented DQN.","tags":null,"title":"Lecture 6:  DQN","type":"docs"},{"authors":null,"categories":null,"content":" Video \u0026nbsp; \u0026nbsp;\nDescription Please note: since last teaching the Policy Gradient content we have discovered two logical errors. Will be updating \u0026amp; reposting the content soon. Stay tuned.\nIn previous lectures, you were introduced to DQN - an algorithm that falls under the first major branch of Reinforcement Learning, \u0026ldquo;Value Based Methods\u0026rdquo;. In this lecture, we introduce you to \u0026ldquo;Policy Gradient methods\u0026rdquo; the second major branch of Reinforcement Learning where we learn to manipulate the object we care about the most - the policy - directly.\n\u0026nbsp; \u0026nbsp;\nLecture Slides \u0026hellip;\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 7:\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for lecture 7:\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material \u0026hellip;\n\u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"cc3dc5608c269aed3842f1b80933e063","permalink":"https://starai-course.github.io/course/lecture7/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture7/","section":"course","summary":"Video \u0026nbsp; \u0026nbsp;\nDescription Please note: since last teaching the Policy Gradient content we have discovered two logical errors. Will be updating \u0026amp; reposting the content soon. Stay tuned.\nIn previous lectures, you were introduced to DQN - an algorithm that falls under the first major branch of Reinforcement Learning, \u0026ldquo;Value Based Methods\u0026rdquo;. In this lecture, we introduce you to \u0026ldquo;Policy Gradient methods\u0026rdquo; the second major branch of Reinforcement Learning where we learn to manipulate the object we care about the most - the policy - directly.","tags":null,"title":"Lecture 7:  Policy Gradient Methods","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription Starcraft 2 is a real time strategy game with highly complicated dynamics and rich multi-layered gameplay - which also makes it an ideal environment for AI research. PySC2 is Deepmind\u0026rsquo;s open source library for interfacing with Blizzard\u0026rsquo;s Starcraft 2 game. This session will introduce the PySC2 API, the observation space and the action spaces available \u0026amp; participants will build a simple Q learning agent to play the Move to Beacon minigame provided with PySC2.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Lecture 8 Pysc2 slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for lecture 8:\nLecture 8 Exercise 1: Build a Zerg Bot with PySC2 2.0\nLecture 8 Exercise 2: Create a Protoss Bot Using Raw Observations and Actions in PySC2\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  Steven Brown\u0026rsquo;s Discussion of AlphaStar Alexir Pan\u0026rsquo;s Discussi of the AlphaStar Architecture Deepmind PySC2 Repo StarAi PySC2 Tools  \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"0ec25ee69fc97481fd818d0ca6663540","permalink":"https://starai-course.github.io/course/lecture8/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture8/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription Starcraft 2 is a real time strategy game with highly complicated dynamics and rich multi-layered gameplay - which also makes it an ideal environment for AI research. PySC2 is Deepmind\u0026rsquo;s open source library for interfacing with Blizzard\u0026rsquo;s Starcraft 2 game. This session will introduce the PySC2 API, the observation space and the action spaces available \u0026amp; participants will build a simple Q learning agent to play the Move to Beacon minigame provided with PySC2.","tags":null,"title":"Lecture 8:  PySC2","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription The StarAi team is excited to offer a lecture \u0026amp; exercises on one of the the most cutting edge, end-to-end value based reinforcement learning algorithms out there - Deepmind\u0026rsquo;s rainbow.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Bonus Lecture 1 Rainbow slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for this lecture:\nBonus Rainbow Notebook: Duelling DQN\nBonus Rainbow Notebook: Multi Step DQN\nBonus Rainbow Notebook: Prioritised Experience Replay DQN\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material  Deepmind\u0026rsquo;s Rainbow Paper  \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"7e45c87c586f6fbb3a0f380707d69338","permalink":"https://starai-course.github.io/course/lecture9/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture9/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription The StarAi team is excited to offer a lecture \u0026amp; exercises on one of the the most cutting edge, end-to-end value based reinforcement learning algorithms out there - Deepmind\u0026rsquo;s rainbow.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Bonus Lecture 1 Rainbow slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for this lecture:\nBonus Rainbow Notebook: Duelling DQN\nBonus Rainbow Notebook: Multi Step DQN","tags":null,"title":"Bonus:  Rainbow","type":"docs"},{"authors":null,"categories":null,"content":" Video  \u0026nbsp; \u0026nbsp;\nDescription A2C is an algorithm \u0026ldquo;framework\u0026rdquo; that combines value \u0026amp; policy based methodologies discussed in previous lectures.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Bonus Lecture 2 A2C slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for the A2C lecture:\nBonus Lecture Exercise: Advantage Actor Critic\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for the bonus lecture :\nBonus Exercise Solutions: Advantage Actor Critic\n\u0026nbsp; \u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nAdditional Learning Material \u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"4283ec4799f3707ba5872cb553ade230","permalink":"https://starai-course.github.io/course/lecture10/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture10/","section":"course","summary":"Video  \u0026nbsp; \u0026nbsp;\nDescription A2C is an algorithm \u0026ldquo;framework\u0026rdquo; that combines value \u0026amp; policy based methodologies discussed in previous lectures.\n\u0026nbsp; \u0026nbsp;\nLecture Slides StarAi Bonus Lecture 2 A2C slides\n\u0026nbsp; \u0026nbsp;\nExercise Follow the link below to access the exercises for the A2C lecture:\nBonus Lecture Exercise: Advantage Actor Critic\n\u0026nbsp; \u0026nbsp;\nExercise Solutions Follow the link below to access the exercise solutions for the bonus lecture :","tags":null,"title":"Bonus: the A2C algorithm","type":"docs"},{"authors":null,"categories":null,"content":" \u0026nbsp; \u0026nbsp;\nAdditional Learning Material \u0026nbsp; \u0026nbsp;\nFurther Learnings - Foundations:\nIntro to Reinforcement Learning by David Silver (Deepmind)\nReinforcement Learning: An Introduction (textbook)\nDenny Britz Github Repo\nOpenAi Spinning Up in Deep RL\n\u0026nbsp; \u0026nbsp;\nFurther Learnings - Advanced:\nBerkeley Deep RL Bootcamp\nCS294 Deep Reinforcement Learning (Berkeley)\nDeepmind\u0026rsquo;s IMPALA RL Framework\n\u0026nbsp; \u0026nbsp;\n","date":1554296400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554296400,"objectID":"bf16e98a8aef24aae3466792e146530b","permalink":"https://starai-course.github.io/course/lecture11/","publishdate":"2019-04-04T00:00:00+11:00","relpermalink":"/course/lecture11/","section":"course","summary":"\u0026nbsp; \u0026nbsp;\nAdditional Learning Material \u0026nbsp; \u0026nbsp;\nFurther Learnings - Foundations:\nIntro to Reinforcement Learning by David Silver (Deepmind)\nReinforcement Learning: An Introduction (textbook)\nDenny Britz Github Repo\nOpenAi Spinning Up in Deep RL\n\u0026nbsp; \u0026nbsp;\nFurther Learnings - Advanced:\nBerkeley Deep RL Bootcamp\nCS294 Deep Reinforcement Learning (Berkeley)\nDeepmind\u0026rsquo;s IMPALA RL Framework\n\u0026nbsp; \u0026nbsp;","tags":null,"title":"Bonus: Additional Learning Material","type":"docs"},{"authors":null,"categories":null,"content":"Way back in April 2018! -an actual epoch in the machine learning world!- we set out to create this course to help speed up the development of advanced machine learning techniques through a fun platform - mainly - Starcraft 2, hence the name \u0026ldquo;StarAi\u0026rdquo;.\nFor a long time Starcraft has been considered within the machine learning community to be the next \u0026ldquo;Grand Challenge\u0026rdquo; for Artificial Intelligence due to several properties of the game including very high state \u0026amp; action spaces, partial observability \u0026amp; multi agent gameplay.\nThis year Deepmind made significant progress with regards to solving the game. For all intents and purposes, removing the \u0026ldquo;human limitations\u0026rdquo; restrictions currently emposed on Deepmind\u0026rsquo;s system, the game can be said to be solved and am sure we will see even further progress towards the end of the year from Deepmind\u0026rsquo;s Starcraft team akin to AlphaGo Zero.\nWhy release the course then? We believe that the way we taught the content - through stories - is unique, and a fun way to learn advanced machine learning \u0026amp; reinforcement learning techniques in a \u0026ldquo;human\u0026rdquo; way. We would also be honoured if someone out there takes inspiration from this course that eventually leads to new breakthroughs!\nWe also believe that RL will have massive impact in the real world in end to end machine learning problems.\nFor those of you only interested in the Starcraft element of StarAi please refer to the content of week 6 by clicking this link.\n","date":1483189200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483189200,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://starai-course.github.io/about/","publishdate":"2017-01-01T00:00:00+11:00","relpermalink":"/about/","section":"","summary":"Way back in April 2018! -an actual epoch in the machine learning world!- we set out to create this course to help speed up the development of advanced machine learning techniques through a fun platform - mainly - Starcraft 2, hence the name \u0026ldquo;StarAi\u0026rdquo;.\nFor a long time Starcraft has been considered within the machine learning community to be the next \u0026ldquo;Grand Challenge\u0026rdquo; for Artificial Intelligence due to several properties of the game including very high state \u0026amp; action spaces, partial observability \u0026amp; multi agent gameplay.","tags":null,"title":"About.","type":"page"},{"authors":null,"categories":null,"content":" StarAi is a team of Developers, Machine Learning Engineers \u0026amp; Researchers based in Australia. \u0026nbsp; \u0026nbsp;\nInstructor Team:\n\u0026nbsp; \u0026nbsp;\nMachine Learning Engineer Paul Conyngham\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nReinforcement Learning PhD Candidate Alex Long\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nSoftware Engineer William Xu\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nSoftware Engineer Artem Golubev\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nArtificial Intelligence Engineer Steven Brown\n\u0026nbsp;\n\u0026nbsp;\nTeaching Assistants:\n\u0026nbsp; \u0026nbsp;\nMachine Learning Engineer Ben Jelliffe\n\u0026nbsp;\n\u0026nbsp;\nDeveloper Team:\n\u0026nbsp; \u0026nbsp;\nLow Level Software Engineer Franklin He\n\u0026nbsp;\n\u0026nbsp;\nVideo Editing Team:\n\u0026nbsp; \u0026nbsp;\nVideo Editor \u0026amp; ITS Engineer Miguel Costa\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nVideo Editor Michael DiMola III\n\u0026nbsp;\n\u0026nbsp;\n","date":1483189200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483189200,"objectID":"3bf44c81f2a197de01edde90bcd77783","permalink":"https://starai-course.github.io/team/","publishdate":"2017-01-01T00:00:00+11:00","relpermalink":"/team/","section":"","summary":"StarAi is a team of Developers, Machine Learning Engineers \u0026amp; Researchers based in Australia. \u0026nbsp; \u0026nbsp;\nInstructor Team:\n\u0026nbsp; \u0026nbsp;\nMachine Learning Engineer Paul Conyngham\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nReinforcement Learning PhD Candidate Alex Long\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nSoftware Engineer William Xu\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nSoftware Engineer Artem Golubev\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp; \u0026nbsp;\nArtificial Intelligence Engineer Steven Brown\n\u0026nbsp;\n\u0026nbsp;\nTeaching Assistants:\n\u0026nbsp; \u0026nbsp;\nMachine Learning Engineer Ben Jelliffe","tags":null,"title":"Team","type":"page"},{"authors":null,"categories":null,"content":" We would like to accelerate the progress of Reinforcement Learning research. We believe that getting the public involved in RL is one way to do this, however not everyone has access to a powerful computer in order to perform state of the art machine learning. Early last year, the StarAi team succeeded in getting Google Deepmind\u0026rsquo;s PySc2 to run on Colaboratory. This translates to powerful free compute for all. \u0026nbsp; \u0026nbsp;\n\u0026nbsp;\nStarAi Starcraft Google Colaboratory IPython Notebook\nhttps://colab.research.google.com/drive/1AzCKV98UaQQz2aJIeGWlExcxBrpgKsIV\nAuthor: Frank He\n\u0026nbsp;\n\u0026nbsp;\nStarAi tutorial on how to setup your Starcraft machine learning model for Colaboratory\nhttps://medium.com/@paul.steven.conyngham/how-to-optimise-your-starcraft-machine-learning-model-for-google-colaboratory-5525d5aed01e\nAuthor: Paul Conyngham\n\u0026nbsp;\n\u0026nbsp;\nStarAi: \u0026ldquo;Foundations\u0026rdquo; modular Reinforcement Learning Framework\nhttps://github.com/star-ai/foundations\nAuthor: William Xu\n\u0026nbsp;\n","date":1483189200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483189200,"objectID":"b51dc600979d5dc62730cc9d9715551a","permalink":"https://starai-course.github.io/tools/","publishdate":"2017-01-01T00:00:00+11:00","relpermalink":"/tools/","section":"","summary":"We would like to accelerate the progress of Reinforcement Learning research. We believe that getting the public involved in RL is one way to do this, however not everyone has access to a powerful computer in order to perform state of the art machine learning. Early last year, the StarAi team succeeded in getting Google Deepmind\u0026rsquo;s PySc2 to run on Colaboratory. This translates to powerful free compute for all. \u0026nbsp; \u0026nbsp;","tags":null,"title":"Tools","type":"page"}]