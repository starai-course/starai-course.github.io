<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>StarAi Deep Reinforcement Learning Course on StarAi Deep Reinforcement Learning Course</title>
    <link>https://starai-course.github.io/</link>
    <description>Recent content in StarAi Deep Reinforcement Learning Course on StarAi Deep Reinforcement Learning Course</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Apr 2019 00:00:00 +1100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Lecture 0 : Introduction to Reinforcement Learning</title>
      <link>https://starai-course.github.io/course/lecture0/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture0/</guid>
      <description>

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/JYKmkQp7YSg&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;In this lecture, we will take you on a journey into the near future by discussing the recent developments in the field of Reinforcement Learning - by introducing you to what Reinforcement Learning is, how it differs from Deep Learning and the future impact of RL technology.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html&#34; target=&#34;_blank&#34;&gt;Andrej Karpathy&amp;rsquo;s ConvNetJS Deep Q Learning Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 1: Epsilon-Greedy &amp; the multiarmed bandit problem</title>
      <link>https://starai-course.github.io/course/lecture1/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture1/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/jjmkX3QQ-7I&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;In this lecture, we introduce you to your very first RL algorithm, Epsilon Greedy.  We start off by exploring a toy problem known as the &amp;ldquo;multi armed bandit problem&amp;rdquo; or in english- how to win at slot machines! We then dive down into how Epsilon-Greedy solves the bandit problem, go on a detour introducing OpenAi Gym (and why it is important!) and finally hand you over to your first exercise, solving the bandit problem in OpenAi Gym.&lt;/p&gt;

&lt;p&gt;A written version of this lecture is also &lt;a href=&#34;https://star-ai.github.io/What-the-Shell-is-a-Multi-Armed-Bandit-Guest-Starring-the-Epsilon-Greedy-Algorithm/&#34; target=&#34;_blank&#34;&gt;available on the StarAi blog.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Lecture%201_%20Epsilon%20Greedy.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Lecture 1 Epsilon Greedy Lecture slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for lecture 1:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ORJ1aJExBgJdaFQqLWP3V2F3AG25TZnJ&#34; target=&#34;_blank&#34;&gt;Lecture 1: Epsilon-Greedy &amp;amp; the Multi- Armed Bandit with OpenAi Gym&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise-solutions&#34;&gt;Exercise Solutions&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercise solutions for lecture 1:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ORJ1aJExBgJdaFQqLWP3V2F3AG25TZnJ&#34; target=&#34;_blank&#34;&gt;Exercise Solutions: Epsilon-Greedy &amp;amp; the Multi- Armed Bandit with OpenAi Gym&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34;&gt;Sutton &amp;amp; Barto&amp;rsquo;s Reinforcement Learning: An Introduction&lt;/a&gt; - Chapter 2 intro, section 2.1 up to 2.5&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bl.ocks.org/puzzler10/f8955e779fd81f5cf7befaef76bdf503&#34; target=&#34;_blank&#34;&gt;Tom Roth&amp;rsquo;s Multiarmed Bandit Simulator&lt;/a&gt; - Get a feel for how the Multiarmed Bandit works live in your browser!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.edx.org/course/introduction-to-computer-science-and-programming-using-python-2&#34; target=&#34;_blank&#34;&gt;Edx&amp;rsquo;s Brilliant Python Course&lt;/a&gt; - Note that the majority of StarAi&amp;rsquo;s exercises are in the Python programming language. If you would like to further you knowledge in this field we strongly suggest you learn Python.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 2: Markov Decision Processes, Dynamic Programming</title>
      <link>https://starai-course.github.io/course/lecture2/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture2/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/8OHd1MZ8Gxc&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;Disclaimer: This is the most mathematical lecture out of the StarAi series. Whilst we endeavoured to make the StarAi content as accessible as possible, this particular lecture covers the base fundamentals &amp;amp; therefore contains the most formulas. If formulas is not for you please proceed to week 3. If however you would like to dive deeper down the mathematical formulation of the RL framework we highly recommend &lt;a href=&#34;https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&#34; target=&#34;_blank&#34;&gt;David Silver&amp;rsquo;s excellent course on Youtube.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this lecture you will learn the fundamentals of Reinforcement Learning. We start off by discussing the Markov environment and its properties, gradually building our understanding of the intuition behind the Markov Decision Process and its elements, like state-value function, action-value function and policies. We then move on to discussing Bellman equations and the intuition behind them.  At the end we will explore one of the Bellman equation implementations, using the Dynamic Programming approach and finish with an exercise, where you will implement state-value and action-value functions algorithms and find  an optimal policy to solve the Gridworld problem.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Lecture%202_%20Markov%20Decision%20Processes.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Lecture 2 Markov Decision Processes slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for lecture 2:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1U2S9RT74KroGdYZNmEoS4hPxnpIryrr9&#34; target=&#34;_blank&#34;&gt;Lecture 2 Exercise 1: Policy Evaluation Exercise&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1A5xFYs5wf6RKYY4OF2XYZlnMGreSv0p7&#34; target=&#34;_blank&#34;&gt;Lecture 2 Exercise 2: Policy Iteration Exercise&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1S-YKcZqWdnQFeB7tK3u-_o1MTD15mnFD/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;Lecture 2 Exercise 3: Value Iteration Exercise&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise-solutions&#34;&gt;Exercise Solutions&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercise solutions for lecture 2:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1TRMfAvdkydL8mOPJHr-Jt1pHT4CPx6C0/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;Exercise Solutions 1: Policy Evaluation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1eDKS3FGLgdIFNlrh961CpniPs3Lu9way/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;Exercise Solutions: Value Iteration&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34;&gt;Sutton &amp;amp; Barto&amp;rsquo;s Reinforcement Learning: An Introduction&lt;/a&gt; - All of Chapter 3 &amp;amp; 4.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 3: Tabular Q Gridworld</title>
      <link>https://starai-course.github.io/course/lecture3/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture3/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/cJTqEBZ9l9Q&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;In this session, participants will focus on a specific method of Temporal Difference Learning called Tabular Q Learning. Participants will learn the theory behind Q Learning, implement the different components bit by bit and combine these components to solve the robot in a maze scenario.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Lecture%203%264_TabularQ.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Lecture 3 &amp;amp; 4 TabularQ slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for lecture 3:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ob5CyQxYcL0z15vm6oHjAitQOoIbLvB5&#34; target=&#34;_blank&#34;&gt;Lecture 3 Exercise: Tabular Q FrozenLake&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise-solutions&#34;&gt;Exercise Solutions&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercise solutions for lecture 3:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1wxusmHIWWAgOlRCQc-nMxc-iudFnkURA&#34; target=&#34;_blank&#34;&gt;Exercise Solutions: Tabular Q FrozenLake&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34;&gt;Sutton &amp;amp; Barto&amp;rsquo;s Reinforcement Learning: An Introduction&lt;/a&gt; - Chapter 5 read intro and summary, Chapter 6 intro, section 6.1 to 6.3 + 6.5 (other sections optional but good to read), Chapter 7 read intro and summary&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 4:  Tabular Q Cartpole</title>
      <link>https://starai-course.github.io/course/lecture4/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture4/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/BL6B1qzpaTU&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;In this session, participants will explore the problem of an environment where observations are continuous variables. Participants will learn the discretisation technique and implement this with the previous components to solve the problem of keeping a cart pole upright without having any understanding of the observations.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Lecture%203%264_TabularQ.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Lecture 3 &amp;amp; 4 TabularQ slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for lecture 4:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1fFjj7gymPBYDdMgq7tQR0boWAKArIYMs&#34; target=&#34;_blank&#34;&gt;Lecture 4 Exercise: Tabular Q Cartpole!&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise-solutions&#34;&gt;Exercise Solutions&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercise solutions for lecture 4:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1vezRYQ2qXY7b9x-VwHpxG7nNuNr-iw0x&#34; target=&#34;_blank&#34;&gt;Exercise Solutions: Tabular Q Cartpole&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 5:  NQL Theory</title>
      <link>https://starai-course.github.io/course/lecture5/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture5/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/EfNVUECK6H4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;Neural Q-Learning builds on the theory developed in previous sessions, augmenting the tabular Q-Learning algorithm with the powerful function approximation capabilities of Neural Networks. NQL is the &amp;ldquo;base&amp;rdquo; algorithm unifying Neural Networks and Reinforcement Learning, and participants will be exposed to both the impressive generalization properties of this algorithm, as well as some of it&amp;rsquo;s potential drawbacks and limitations.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Lecture%205-part%201-Neural%20Q%20Theory.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Lecture 5 part 1 Neural Q Theory slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Lecture%205-part%202-Neural%20Q%20Implementation.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Lecture 5 part 2 Neural Q Implementation slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for lecture 5:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1n3BLoAjqEdOkXbW0hPLzHyDnT-xVkTil&#34; target=&#34;_blank&#34;&gt;lecture 5 Exercise: Neural Q Learning Exercise&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise-solutions&#34;&gt;Exercise Solutions&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercise solutions for lecture 5:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1f0FzLwFspcMEgHdRkjV2NcA4IvQWOJ50&#34; target=&#34;_blank&#34;&gt;lecture 5 Exercise: Neural Q Learning Exercise Solutions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34;&gt;Sutton &amp;amp; Barto&amp;rsquo;s Reinforcement Learning: An Introduction&lt;/a&gt; - Chapter 16 section 16.5&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 6:  DQN</title>
      <link>https://starai-course.github.io/course/lecture6/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture6/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/KyEjdezzPwE&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;Deep Q-Networks refer to the method proposed by Deepmind in 2014 to learn to play ATARI2600 games from the raw pixel observations. This hugely influential method kick-started the resurgence in interest in Deep Reinforcement Learning, however it&amp;rsquo;s core contributions deal simply with the stabilization of the NQL algorithm. In these session these key innovations (Experience Replay, Target Networks, and Huber Loss) are stepped though, taking the participants from the relatively unstable NQL algorithm to a fully-implemented DQN.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Lecture%206-DQN.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Lecture 6-DQN slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for lecture 6:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1JMtTOmHLG8xqdfhKm0z3i1N4t4lbXh1P/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;lecture 6 Exercise: DQN Homework Exercise&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise-solutions&#34;&gt;Exercise Solutions&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercise solutions for lecture 6:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1YlgZfVteeLFGnGbsD56vE1dsUwyDreYg/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;lecture 6 Exercise: DQN Homework Exercise Solutions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fevMOp5TDQs&#34; target=&#34;_blank&#34;&gt;DeepRL Bootcamp lecture from Vlad Mnih, one of the original authors of DQN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 7:  Policy Gradient Methods</title>
      <link>https://starai-course.github.io/course/lecture7/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture7/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/P6699vz/Screen-Shot-2019-06-23-at-2-26-48-pm.png&#34; alt=&#34;IMAGE ALT TEXT HERE&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;Please note: since last teaching the Policy Gradient content we have discovered two logical errors. Will be updating &amp;amp; reposting the content soon. Stay tuned.&lt;/p&gt;

&lt;p&gt;In previous lectures, you were introduced to DQN - an algorithm that falls under the first major branch of Reinforcement Learning, &amp;ldquo;Value Based Methods&amp;rdquo;. In this lecture, we introduce you to &amp;ldquo;Policy Gradient methods&amp;rdquo; the second major branch of Reinforcement Learning where we learn to manipulate the object we care about the most - the policy - directly.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for lecture 7:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise-solutions&#34;&gt;Exercise Solutions&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercise solutions for lecture 7:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 8:  PySC2</title>
      <link>https://starai-course.github.io/course/lecture8/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture8/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/videoseries?list=PL99pkap-yVqWBYRlGJ74Eqv184MA9yHE5&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;Starcraft 2 is a real time strategy game with highly complicated dynamics and rich multi-layered gameplay - which also makes it an ideal environment for AI research. PySC2 is Deepmind&amp;rsquo;s open source library for interfacing with Blizzard&amp;rsquo;s Starcraft 2 game. This session will introduce the PySC2 API, the observation space and the action spaces available &amp;amp; participants will build a simple Q learning agent to play the Move to Beacon minigame provided with PySC2.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Lecture%208-Pysc2.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Lecture 8 Pysc2 slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for lecture 8:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://itnext.io/build-a-zerg-bot-with-pysc2-2-0-295375d2f58e&#34; target=&#34;_blank&#34;&gt;Lecture 8 Exercise 1: Build a Zerg Bot with PySC2 2.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://itnext.io/create-a-protoss-bot-using-raw-observations-and-actions-in-pysc2-615f41aa283e&#34; target=&#34;_blank&#34;&gt;Lecture 8 Exercise 2: Create a Protoss Bot Using Raw Observations and Actions in PySC2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://itnext.io/the-evolution-of-alphastar-cefff389b9d5&#34; target=&#34;_blank&#34;&gt;Steven Brown&amp;rsquo;s Discussion of AlphaStar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alexirpan.com/2019/02/22/alphastar.html&#34; target=&#34;_blank&#34;&gt;Alexir Pan&amp;rsquo;s Discussi of the AlphaStar Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/pysc2&#34; target=&#34;_blank&#34;&gt;Deepmind PySC2 Repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.starai.io/tools/&#34; target=&#34;_blank&#34;&gt;StarAi PySC2 Tools&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bonus:  Rainbow</title>
      <link>https://starai-course.github.io/course/lecture9/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture9/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/videoseries?list=PL99pkap-yVqWPxtjMTLHXG5zldr8tcEzq&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;The StarAi team is excited to offer a lecture &amp;amp; exercises on one of the the most cutting edge, end-to-end value based reinforcement learning algorithms out there - Deepmind&amp;rsquo;s rainbow.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Bonus_Lecture%201-Rainbow.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Bonus Lecture 1 Rainbow slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for this lecture:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1pETREgayPYnZTGbI0qWkMXmAvFMZuGJE/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;Bonus Rainbow Notebook: Duelling DQN&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1H9KaNlgl-WZvhlSF3FI15vY1v1KHeesC/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;Bonus Rainbow Notebook: Multi Step DQN&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1e7-L-t-68Ht3DsKanHcJKKE7ytF6d0W5/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;Bonus Rainbow Notebook: Prioritised Experience Replay DQN&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.02298.pdf&#34; target=&#34;_blank&#34;&gt;Deepmind&amp;rsquo;s Rainbow Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bonus: the A2C algorithm</title>
      <link>https://starai-course.github.io/course/lecture10/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture10/</guid>
      <description>

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/videoseries?list=PL99pkap-yVqWwPhUiV_QAmMTDK3oPhzcb&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;A2C is an algorithm &amp;ldquo;framework&amp;rdquo; that combines value &amp;amp; policy based methodologies discussed in previous lectures.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;lecture-slides&#34;&gt;Lecture Slides&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi_%20Bonus_Lecture%202-A2C.pdf&#34; target=&#34;_blank&#34;&gt;StarAi Bonus Lecture 2 A2C slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercises for the A2C lecture:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/12yQCP7yJ69lV4k2x5FY9fJVpY5ktB5IC&#34; target=&#34;_blank&#34;&gt;Bonus Lecture Exercise: Advantage Actor Critic&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;exercise-solutions&#34;&gt;Exercise Solutions&lt;/h2&gt;

&lt;p&gt;Follow the link below to access the exercise solutions for the bonus lecture :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1gkOMzvtuzd6mBbwkwGLvDDtTB140TUKH/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;Bonus Exercise Solutions: Advantage Actor Critic&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;p&gt;Further Learnings - Foundations:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&#34; target=&#34;_blank&#34;&gt;Intro to Reinforcement Learning by David Silver (Deepmind)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://incompleteideas.net/book/bookdraft2018mar21.pdf&#34; target=&#34;_blank&#34;&gt;Reinforcement Learning: An Introduction (textbook)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dennybritz/reinforcement-learning&#34; target=&#34;_blank&#34;&gt;Denny Britz Github Repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Further Learnings - Advanced:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sites.google.com/view/deep-rl-bootcamp/lectures&#34; target=&#34;_blank&#34;&gt;Berkeley Deep RL Bootcamp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://rll.berkeley.edu/deeprlcourse/&#34; target=&#34;_blank&#34;&gt;CS294 Deep Reinforcement Learning (Berkeley)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bonus: Additional Learning Material</title>
      <link>https://starai-course.github.io/course/lecture11/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/course/lecture11/</guid>
      <description>

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;additional-learning-material&#34;&gt;Additional Learning Material&lt;/h2&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Further Learnings - Foundations:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&#34; target=&#34;_blank&#34;&gt;Intro to Reinforcement Learning by David Silver (Deepmind)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://incompleteideas.net/book/bookdraft2018mar21.pdf&#34; target=&#34;_blank&#34;&gt;Reinforcement Learning: An Introduction (textbook)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dennybritz/reinforcement-learning&#34; target=&#34;_blank&#34;&gt;Denny Britz Github Repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://spinningup.openai.com/en/latest/&#34; target=&#34;_blank&#34;&gt;OpenAi Spinning Up in Deep RL&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Further Learnings - Advanced:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sites.google.com/view/deep-rl-bootcamp/lectures&#34; target=&#34;_blank&#34;&gt;Berkeley Deep RL Bootcamp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://rll.berkeley.edu/deeprlcourse/&#34; target=&#34;_blank&#34;&gt;CS294 Deep Reinforcement Learning (Berkeley)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/&#34; target=&#34;_blank&#34;&gt;Deepmind&amp;rsquo;s IMPALA RL Framework&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About.</title>
      <link>https://starai-course.github.io/about/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/about/</guid>
      <description>&lt;p style=&#34;margin-top: 20px;&#34;&gt; &lt;img style=&#34;padding: 0 15px; float: right;&#34; src=https://i.ibb.co/Jv7SzQQ/pnp.png&#34;&gt; &lt;/p&gt;

&lt;p&gt;Way back in April 2018! -an actual epoch in the machine learning world!- we set out to create this course to help speed up the development of advanced machine learning techniques through a fun platform - mainly - Starcraft 2, hence the name &amp;ldquo;StarAi&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://aiandgames.com/zerg-rush-starcraft-ai/&#34; target=&#34;_blank&#34;&gt;For a long time Starcraft has been considered within the machine learning community to be the next &amp;ldquo;Grand Challenge&amp;rdquo; for Artificial Intelligence&lt;/a&gt; due to several properties of the game including very high state &amp;amp; action spaces, partial observability &amp;amp; multi agent gameplay.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/&#34; target=&#34;_blank&#34;&gt;This year Deepmind made significant progress with regards to solving the game&lt;/a&gt;. For all intents and purposes, removing the &amp;ldquo;human limitations&amp;rdquo; restrictions currently emposed on Deepmind&amp;rsquo;s system, the game can be said to be solved and am sure we will see even further progress towards the end of the year from Deepmind&amp;rsquo;s Starcraft team akin to &lt;a href=&#34;https://deepmind.com/blog/alphago-zero-learning-scratch/&#34; target=&#34;_blank&#34;&gt;AlphaGo Zero&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Why even release the course then? We believe that the development of bots for games is a fun a way to learn advanced machine learning techniques &amp;amp; if your work in the field eventually leads to new breakthroughs, even more kudos to you!&lt;/p&gt;

&lt;p&gt;We also believe that RL will have massive impact in the real world in end to end machine learning problems.&lt;/p&gt;

&lt;p&gt;For those of you only interested in the Starcraft element of StarAi &lt;a href=&#34;https://www.starai.io/course/lecture8/&#34; target=&#34;_blank&#34;&gt;please refer to the content of week 6 by clicking this link.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Team</title>
      <link>https://starai-course.github.io/team/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/team/</guid>
      <description>

&lt;h3 id=&#34;starai-is-a-team-of-developers-machine-learning-engineers-researchers-based-in-australia&#34;&gt;StarAi is a team of Developers, Machine Learning Engineers &amp;amp; Researchers based in Australia.&lt;/h3&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Instructor Team:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/98mP9c7/paulie.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Machine Learning Engineer&lt;/strong&gt;  &lt;a href=&#34;https://www.linkedin.com/in/paustevenlconyngham/&#34; target=&#34;_blank&#34;&gt;Paul Conyngham&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/gwQNRWC/longie.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reinforcement Learning PhD Candidate&lt;/strong&gt;  &lt;a href=&#34;https://www.linkedin.com/in/alex-long-rl/&#34; target=&#34;_blank&#34;&gt;Alex Long&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/kQKJywx/will.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Software Engineer&lt;/strong&gt;  &lt;a href=&#34;https://www.linkedin.com/in/william-xu-yuzhou/&#34; target=&#34;_blank&#34;&gt;William Xu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/tBGML5J/artem.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Software Engineer&lt;/strong&gt;  &lt;a href=&#34;https://www.linkedin.com/in/artgo/&#34; target=&#34;_blank&#34;&gt;Artem Golubev&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/DGwzcj4/steven.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Artificial Intelligence Engineer&lt;/strong&gt;  &lt;a href=&#34;https://www.linkedin.com/in/theskjb/&#34; target=&#34;_blank&#34;&gt;Steven Brown&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Teaching Assistants:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/k2qsSzd/ben.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Machine Learning Engineer&lt;/strong&gt;  Ben Jelliffe&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Developer Team:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/CtHBcm9/frankie.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low Level Software Engineer&lt;/strong&gt;  &lt;a href=&#34;https://www.linkedin.com/in/franklin-h-804b85a0/&#34; target=&#34;_blank&#34;&gt;Franklin He&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Video Editing Team:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/Vgk50vr/Screen-Shot-2019-06-17-at-8-24-25-am.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Video Editor &amp;amp; ITS Engineer&lt;/strong&gt;  &lt;a href=&#34;https://www.linkedin.com/in/miguel-costa-technology/&#34; target=&#34;_blank&#34;&gt;Miguel Costa&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/52Pqddr/Screen-Shot-2019-06-17-at-8-24-12-am.png&#34;
     alt=&#34;Markdown Monster icon&#34;
     style=&#34;float: left; margin-right: 10px;&#34;  /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Video Editor&lt;/strong&gt;  &lt;a href=&#34;https://www.linkedin.com/in/michael-dimola-iii-61b863179/&#34; target=&#34;_blank&#34;&gt;Michael DiMola III&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Tools</title>
      <link>https://starai-course.github.io/tools/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +1100</pubDate>
      
      <guid>https://starai-course.github.io/tools/</guid>
      <description>

&lt;h3 id=&#34;we-would-like-to-accelerate-the-progress-of-reinforcement-learning-research&#34;&gt;We would like to accelerate the progress of Reinforcement Learning research.&lt;/h3&gt;

&lt;h3 id=&#34;we-believe-that-getting-the-public-involved-in-rl-is-one-way-to-do-this-however-not-everyone-has-access-to-a-powerful-computer-in-order-to-perform-state-of-the-art-machine-learning&#34;&gt;We believe that getting the public involved in RL is one way to do this, however not everyone has access to a powerful computer in order to perform state of the art machine learning.&lt;/h3&gt;

&lt;h3 id=&#34;early-last-year-the-starai-team-succeeded-in-getting-google-deepmind-s-pysc2-to-run-on-colaboratory&#34;&gt;Early last year, the StarAi team succeeded in getting Google Deepmind&amp;rsquo;s PySc2 to run on Colaboratory.&lt;/h3&gt;

&lt;h3 id=&#34;this-translates-to-powerful-free-compute-for-all&#34;&gt;This translates to powerful free compute for all.&lt;/h3&gt;

&lt;p&gt;&amp;nbsp;
&amp;nbsp;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;StarAi Starcraft Google Colaboratory IPython Notebook&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1AzCKV98UaQQz2aJIeGWlExcxBrpgKsIV&#34; target=&#34;_blank&#34;&gt;https://colab.research.google.com/drive/1AzCKV98UaQQz2aJIeGWlExcxBrpgKsIV&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Frank He&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;StarAi tutorial on how to setup your Starcraft machine learning model for Colaboratory&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/@paul.steven.conyngham/how-to-optimise-your-starcraft-machine-learning-model-for-google-colaboratory-5525d5aed01e&#34; target=&#34;_blank&#34;&gt;https://medium.com/@paul.steven.conyngham/how-to-optimise-your-starcraft-machine-learning-model-for-google-colaboratory-5525d5aed01e&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Paul Conyngham&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;StarAi: &amp;ldquo;Foundations&amp;rdquo; modular Reinforcement Learning Framework&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/star-ai/foundations&#34; target=&#34;_blank&#34;&gt;https://github.com/star-ai/foundations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; William Xu&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
